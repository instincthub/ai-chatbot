{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Free RAG Implementation\n",
    "## Minimal Dependencies Version\n",
    "\n",
    "This notebook provides a simpler RAG implementation that avoids complex dependency issues:\n",
    "- Uses basic langchain components\n",
    "- Minimal dependencies\n",
    "- Works on Windows without compilation issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install minimal required packages\n",
    "# Run this cell if packages are not installed\n",
    "%pip install langchain langchain-community pypdf chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import minimal required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.base import Embeddings\n",
    "import hashlib\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Simple embeddings initialized!\n",
      "ðŸ“Š Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Simple custom embeddings class (for demonstration)\n",
    "# This creates basic embeddings without external dependencies\n",
    "class SimpleEmbeddings(Embeddings):\n",
    "    \"\"\"Simple embeddings using hash-based vectors for demonstration\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim=384):\n",
    "        self.embedding_dim = embedding_dim\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"Embed a list of documents\"\"\"\n",
    "        return [self._embed_text(text) for text in texts]\n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        \"\"\"Embed a query text\"\"\"\n",
    "        return self._embed_text(text)\n",
    "    \n",
    "    def _embed_text(self, text):\n",
    "        \"\"\"Create a simple embedding from text using hashing\"\"\"\n",
    "        # This is a simple demonstration - in production use real embeddings\n",
    "        # Create deterministic \"embedding\" from text\n",
    "        hash_digest = hashlib.sha256(text.encode()).digest()\n",
    "        # Convert to float array\n",
    "        embedding = np.frombuffer(hash_digest, dtype=np.uint8).astype(np.float32)\n",
    "        # Normalize and resize to embedding_dim\n",
    "        embedding = embedding / 255.0\n",
    "        # Repeat or truncate to match embedding_dim\n",
    "        if len(embedding) < self.embedding_dim:\n",
    "            embedding = np.pad(embedding, (0, self.embedding_dim - len(embedding)))\n",
    "        else:\n",
    "            embedding = embedding[:self.embedding_dim]\n",
    "        return embedding.tolist()\n",
    "\n",
    "# Initialize simple embeddings\n",
    "embeddings = SimpleEmbeddings()\n",
    "print(\"âœ… Simple embeddings initialized!\")\n",
    "print(f\"ðŸ“Š Embedding dimension: {embeddings.embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š Document processing functions ready!\n"
     ]
    }
   ],
   "source": [
    "# Document loading functions\n",
    "def read_doc(directory):\n",
    "    \"\"\"Load PDF documents from directory\"\"\"\n",
    "    file_loader = PyPDFDirectoryLoader(directory)\n",
    "    documents = file_loader.load()\n",
    "    return documents\n",
    "\n",
    "def chunk_data(docs, chunk_size=800, chunk_overlap=50):\n",
    "    \"\"\"Split documents into chunks\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    return chunks\n",
    "\n",
    "print(\"ðŸ“š Document processing functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š Loading documents...\n",
      "âœ… Loaded 28 documents\n",
      "âœ‚ï¸ Splitting documents into chunks...\n",
      "âœ… Created 45 document chunks\n"
     ]
    }
   ],
   "source": [
    "# Load and process documents\n",
    "print(\"ðŸ“š Loading documents...\")\n",
    "doc = read_doc('documents/')\n",
    "print(f\"âœ… Loaded {len(doc)} documents\")\n",
    "\n",
    "print(\"âœ‚ï¸ Splitting documents into chunks...\")\n",
    "documents = chunk_data(docs=doc)\n",
    "print(f\"âœ… Created {len(documents)} document chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Creating local vector store...\n",
      "â³ This may take a few minutes for large documents...\n",
      "âœ… Vector store created successfully!\n",
      "ðŸ“Š Indexed 45 document chunks\n"
     ]
    }
   ],
   "source": [
    "# Create local vector store using Chroma\n",
    "print(\"ðŸ”„ Creating local vector store...\")\n",
    "print(\"â³ This may take a few minutes for large documents...\")\n",
    "\n",
    "# Chroma will store vectors locally\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Vector store created successfully!\")\n",
    "print(f\"ðŸ“Š Indexed {len(documents)} document chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸ Retrieval functions ready!\n"
     ]
    }
   ],
   "source": [
    "# Simple retrieval function\n",
    "def retrieve_similar_chunks(query, k=3):\n",
    "    \"\"\"Retrieve k most similar document chunks\"\"\"\n",
    "    results = vectorstore.similarity_search(query, k=k)\n",
    "    return results\n",
    "\n",
    "# Simple answer generation (without LLM)\n",
    "def generate_simple_answer(query, relevant_chunks):\n",
    "    \"\"\"Generate a simple answer by extracting relevant sentences\"\"\"\n",
    "    print(f\"\\nðŸ” Query: {query}\")\n",
    "    print(f\"ðŸ“š Found {len(relevant_chunks)} relevant chunks\\n\")\n",
    "    \n",
    "    # Extract sentences containing query keywords\n",
    "    query_words = query.lower().split()\n",
    "    relevant_sentences = []\n",
    "    \n",
    "    for chunk in relevant_chunks:\n",
    "        sentences = chunk.page_content.split('.')\n",
    "        for sentence in sentences:\n",
    "            if any(word in sentence.lower() for word in query_words):\n",
    "                relevant_sentences.append(sentence.strip())\n",
    "    \n",
    "    # Return unique sentences\n",
    "    unique_sentences = list(dict.fromkeys(relevant_sentences))\n",
    "    return '. '.join(unique_sentences[:3]) + '.'\n",
    "\n",
    "print(\"ðŸ› ï¸ Retrieval functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Testing Simple RAG System\n",
      "==================================================\n",
      "\n",
      "ðŸ” Query: What are the available products?\n",
      "ðŸ“š Found 3 relevant chunks\n",
      "\n",
      "\n",
      "ðŸ“‹ ANSWER:\n",
      "--------------------\n",
      "Given the increasing demand for healthcare services and the inadequacy of Government's intervention in \n",
      "the sector, private sector participation has been on the rise across the entire value chain of the sector. Consequently, greater emphasis is placed on the growing private healthcare  market, which makes it a \n",
      "focal point for prospective investors to examine. Our goal in the sector is to:\n",
      "HEALTH\n",
      "Improve healthcare delivery \n",
      "infrastructure through Equipment \n",
      "Financing\n",
      "Improve access to medical technology that also improves \n",
      "health business by providing access and information to \n",
      "practitioners in the sector.\n",
      "\n",
      "ðŸ“š SOURCE CHUNKS:\n",
      "------------------------------\n",
      "\n",
      "ðŸ“„ Chunk 1:\n",
      "Source: documents\\At-The-HEART-of-Sterling.pdf\n",
      "Page: 17\n",
      "Preview: Given the increasing demand for healthcare services and the inadequacy of Government's intervention in \n",
      "the sector, private sector participation has been on the rise across the entire value chain of t...\n",
      "\n",
      "ðŸ“„ Chunk 2:\n",
      "Source: documents\\At-The-HEART-of-Sterling.pdf\n",
      "Page: 4\n",
      "Preview: Enhance brand visibility\n",
      "with 'the one-customer\n",
      "bank' slogan\n",
      "Improve\n",
      "technological \n",
      "capability\n",
      "Grow our \n",
      "customer base\n",
      "Fund deposit book \n",
      "predominantly from\n",
      "the retail segment\n",
      "Build a knowledge\n",
      "driven...\n",
      "\n",
      "ðŸ“„ Chunk 3:\n",
      "Source: documents\\At-The-HEART-of-Sterling.pdf\n",
      "Page: 5\n",
      "Preview: Sold non-core businesses \n",
      "following the repeal of \n",
      "universal banking by the \n",
      "CBN. Acquired Equitorial \n",
      "Trust Bank to scale our \n",
      "business\n",
      "Completed \n",
      "integration of ETB \n",
      "and launched retail \n",
      "banking\n",
      "Rai...\n"
     ]
    }
   ],
   "source": [
    "# Test the simple RAG system\n",
    "test_query = \"What are the available products?\"\n",
    "\n",
    "print(\"ðŸš€ Testing Simple RAG System\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Retrieve relevant chunks\n",
    "relevant_chunks = retrieve_similar_chunks(test_query)\n",
    "\n",
    "# Generate simple answer\n",
    "answer = generate_simple_answer(test_query, relevant_chunks)\n",
    "\n",
    "print(\"\\nðŸ“‹ ANSWER:\")\n",
    "print(\"-\" * 20)\n",
    "print(answer)\n",
    "\n",
    "print(\"\\nðŸ“š SOURCE CHUNKS:\")\n",
    "print(\"-\" * 30)\n",
    "for i, chunk in enumerate(relevant_chunks, 1):\n",
    "    print(f\"\\nðŸ“„ Chunk {i}:\")\n",
    "    print(f\"Source: {chunk.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Page: {chunk.metadata.get('page', 'Unknown')}\")\n",
    "    print(f\"Preview: {chunk.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Testing multiple queries:\n",
      "\n",
      "ðŸ” Query: Which bank supports the agriculture sector?\n",
      "ðŸ“š Found 3 relevant chunks\n",
      "\n",
      "\n",
      "â“ Question: Which bank supports the agriculture sector?\n",
      "============================================================\n",
      "\n",
      "ðŸ’¡ Answer: 2006 - 2010 \n",
      "THE BIRTHING PROCESS\n",
      "OUR HERITAGE\n",
      "Establish a foothold\n",
      "to gain market share\n",
      "Create a distinct \n",
      "brand identity. Grow our retail footprint\n",
      "by investing in technology\n",
      "and service channel growth\n",
      "Adopt social media \n",
      "to deepen customer \n",
      "interactions\n",
      "Integrate our people\n",
      "following the M&A\n",
      "Sustainable solutions \n",
      "to reposition us\n",
      "as a key competitor\n",
      "Beef up capital\n",
      "We navigated through these years to:\n",
      "Sterling Bank was born out of a merger of five \n",
      "Nigerian banks in a bid to achieve compliance \n",
      "with the regulatory requirement mandating a \n",
      "N25 billion capital base for Nigerian banks. These banks were predominantly investment \n",
      "banks with little retail footprint.\n",
      "\n",
      "ðŸ“Š Based on 3 relevant chunk(s)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "ðŸ” Query: What is the agriculture credit target?\n",
      "ðŸ“š Found 3 relevant chunks\n",
      "\n",
      "\n",
      "â“ Question: What is the agriculture credit target?\n",
      "============================================================\n",
      "\n",
      "ðŸ’¡ Answer: Sold non-core businesses \n",
      "following the repeal of \n",
      "universal banking by the \n",
      "CBN. Acquired Equitorial \n",
      "Trust Bank to scale our \n",
      "business\n",
      "Completed \n",
      "integration of ETB \n",
      "and launched retail \n",
      "banking\n",
      "Raised N12. 1bn via \n",
      "Rights Issue; \n",
      "Obtained Non-\n",
      "Interest Banking \n",
      "license; Launched \n",
      "Agent Banking\n",
      "Raised US$120m \n",
      "(N19.\n",
      "\n",
      "ðŸ“Š Based on 3 relevant chunk(s)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "ðŸ” Query: What initiatives are mentioned for farmers?\n",
      "ðŸ“š Found 3 relevant chunks\n",
      "\n",
      "\n",
      "â“ Question: What initiatives are mentioned for farmers?\n",
      "============================================================\n",
      "\n",
      "ðŸ’¡ Answer: Å¸ Optimize operations and technology to drive \n",
      "better control, manage costs, complexity and risk\n",
      "Switch: Our goal is to build a one-stop financial hub for \n",
      "customers to initiate and seamlessly complete both \n",
      "financial and non-financial activities online or using \n",
      "mobile devices solely with a minimal number (2/3 max) \n",
      "of clicks/taps. The long-term plan for Switch is to \n",
      "become that intuitive personalized customer solution \n",
      "that is constantly adaptive to the customer's \n",
      "behavioral changes as well as provide an ecosystem \n",
      "for them to experience their lifestyle choices. SAF Retail:  The retail store provides us the \n",
      "opportunity to create access to market for our small \n",
      "and medium enterprises through the marketplace,.\n",
      "\n",
      "ðŸ“Š Based on 3 relevant chunk(s)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "ðŸ” Query: Tell me about fisheries\n",
      "ðŸ“š Found 3 relevant chunks\n",
      "\n",
      "\n",
      "â“ Question: Tell me about fisheries\n",
      "============================================================\n",
      "\n",
      "ðŸ’¡ Answer: Grow our retail footprint\n",
      "by investing in technology\n",
      "and service channel growth\n",
      "Adopt social media \n",
      "to deepen customer \n",
      "interactions\n",
      "Integrate our people\n",
      "following the M&A\n",
      "Sustainable solutions \n",
      "to reposition us\n",
      "as a key competitor\n",
      "Beef up capital\n",
      "We navigated through these years to:\n",
      "Sterling Bank was born out of a merger of five \n",
      "Nigerian banks in a bid to achieve compliance \n",
      "with the regulatory requirement mandating a \n",
      "N25 billion capital base for Nigerian banks. These banks were predominantly investment \n",
      "banks with little retail footprint. Given this \n",
      "fact, the business of commercial banking was \n",
      "somewhat new to Sterling Bank with \n",
      "challenges.\n",
      "\n",
      "ðŸ“Š Based on 3 relevant chunk(s)\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Interactive query function\n",
    "def ask_question(question):\n",
    "    \"\"\"Ask a question and get relevant information\"\"\"\n",
    "    chunks = retrieve_similar_chunks(question, k=3)\n",
    "    answer = generate_simple_answer(question, chunks)\n",
    "    \n",
    "    print(f\"\\nâ“ Question: {question}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nðŸ’¡ Answer: {answer}\")\n",
    "    print(f\"\\nðŸ“Š Based on {len(chunks)} relevant chunk(s)\")\n",
    "    \n",
    "    return answer, chunks\n",
    "\n",
    "# Test with different questions\n",
    "questions = [\n",
    "    \"Which bank supports the agriculture sector?\",\n",
    "    \"What is the agriculture credit target?\",\n",
    "    \"What initiatives are mentioned for farmers?\",\n",
    "    \"Tell me about fisheries\"\n",
    "]\n",
    "\n",
    "print(\"ðŸŽ¯ Testing multiple queries:\")\n",
    "for q in questions:\n",
    "    ask_question(q)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Simple RAG System Summary\n",
    "\n",
    "### âœ… What This Version Provides:\n",
    "- **Minimal Dependencies**: Only uses basic langchain and chromadb\n",
    "- **No Compilation Issues**: Works on Windows without Rust/C++ requirements\n",
    "- **Local Storage**: Uses ChromaDB for local vector storage\n",
    "- **Simple Embeddings**: Basic demonstration embeddings (replace with real ones in production)\n",
    "- **No LLM Required**: Simple keyword-based answer extraction\n",
    "\n",
    "### ðŸš€ Upgrading to Production:\n",
    "\n",
    "1. **Better Embeddings**: Replace SimpleEmbeddings with:\n",
    "   ```python\n",
    "   # Option 1: Use OpenAI (requires API key)\n",
    "   from langchain.embeddings import OpenAIEmbeddings\n",
    "   embeddings = OpenAIEmbeddings()\n",
    "   \n",
    "   # Option 2: Use local embeddings (when dependencies work)\n",
    "   from langchain.embeddings import HuggingFaceEmbeddings\n",
    "   embeddings = HuggingFaceEmbeddings()\n",
    "   ```\n",
    "\n",
    "2. **Add LLM for Better Answers**:\n",
    "   ```python\n",
    "   # Local LLM (when dependencies work)\n",
    "   from langchain.llms import LlamaCpp\n",
    "   llm = LlamaCpp(model_path=\"./models/llama-2-7b.bin\")\n",
    "   ```\n",
    "\n",
    "3. **Use Pinecone for Cloud Storage**:\n",
    "   ```python\n",
    "   from langchain.vectorstores import Pinecone\n",
    "   vectorstore = Pinecone.from_documents(documents, embeddings)\n",
    "   ```\n",
    "\n",
    "### ðŸ“ Notes:\n",
    "- This is a simplified version for development/testing\n",
    "- The embeddings are basic - use real embeddings for better results\n",
    "- Answer generation is keyword-based - add LLM for natural responses\n",
    "- ChromaDB stores vectors locally in `./chroma_db` directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
