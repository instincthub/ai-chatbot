{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import Libraries\n",
    "\n",
    "import openai\n",
    "import langchain\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, OpenAI\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pinecone index: ai-chatbot-2\n"
     ]
    }
   ],
   "source": [
    "# API Keys\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")  # Fixed: removed comma\n",
    "PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\")\n",
    "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "print(f\"Using Pinecone index: {PINECONE_INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI and Pinecone\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name = PINECONE_INDEX_NAME\n",
    "\n",
    "# Note: We don't need to get the index here since PineconeVectorStore.from_documents will handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x00000251B1025F90>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x00000251B1026AD0>, model='text-embedding-3-small', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Embedding Technique Of OPENAI - Updated to use langchain_openai\n",
    "# Note: If you're getting RateLimitError, check your OpenAI account at https://platform.openai.com/account/usage\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # Updated to use newer embedding model\n",
    "    api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "embeddings\n",
    "\n",
    "# Alternative: Use HuggingFace embeddings (free)\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Read the document\n",
    "def read_doc(directory):\n",
    "    file_loader=PyPDFDirectoryLoader(directory)\n",
    "    documents=file_loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=read_doc('documents/')\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide the docs into chunks - Updated with best practices\n",
    "### https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html#\n",
    "def chunk_data(docs, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split documents into chunks using RecursiveCharacterTextSplitter\n",
    "    Updated with 2024 best practices for chunk size and overlap\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    doc = text_splitter.split_documents(docs)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=chunk_data(docs=doc)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors=embeddings.embed_query(\"How are you?\")\n",
    "# len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store with 43 documents...\n",
      "Vector store created/connected successfully with index: ai-chatbot-2\n"
     ]
    }
   ],
   "source": [
    "# Create Pinecone vector store from documents\n",
    "print(f\"Creating vector store with {len(documents)} documents...\")\n",
    "\n",
    "# Initialize or connect to existing Pinecone vector store\n",
    "vector_store = PineconeVectorStore.from_documents(\n",
    "    documents=documents,  # Use the chunked documents\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "print(f\"Vector store created/connected successfully with index: {index_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cosine Similarity Retrieve Results from VectorDB\n",
    "def retrieve_query(query,k=2):\n",
    "    matching_results=vector_store.similarity_search(query,k=k)\n",
    "    return matching_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modern LLM and QA Chain setup\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.5)\n",
    "\n",
    "# Create a prompt template for QA\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "# Create the document chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Search answers from VectorDB using modern chain\n",
    "def retrieve_answers(query):\n",
    "    # Get relevant documents\n",
    "    doc_search = retrieve_query(query)\n",
    "    print(\"Retrieved documents:\", doc_search)\n",
    "    \n",
    "    # Use the modern document chain\n",
    "    response = document_chain.invoke({\n",
    "        \"context\": doc_search,\n",
    "        \"input\": query\n",
    "    })\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved documents: [Document(id='388d2736-aba7-4bc2-91f9-25bbe7c70cbf', metadata={'author': 'Maurice Igugu', 'creationdate': '2019-03-22T15:41:09+01:00', 'creator': 'CorelDRAW 2019', 'moddate': '2019-03-24T15:34:42+01:00', 'page': 15.0, 'page_label': '16', 'producer': 'Corel PDF Engine Version 21.0.0.593', 'source': 'documents\\\\At-The-HEART-of-Sterling.pdf', 'title': 'At The HEART of Sterling - Old March 2019.cdr', 'total_pages': 28.0}, page_content='era where innovation meets opportunity.\\nOUR TARGET MARKETS\\nOur choice of market \\nsegments is based on \\nthe understanding of \\nemerging trends in \\nthe macroeconomic \\nenvironment and \\nopportunities in the \\nsectors of interest. \\nSecondary\\nMarkets\\nPublic Sector Mining Oil & Gas\\n(Upstream, \\ndownstream\\n& services) \\nTelecom-\\nmunications\\nManufacturing Real Estate Wholesale\\n& Trading\\nPower\\n(Generation &\\nDistribution\\nHealth AgricultureEducation Renewable\\nEnergy\\nTransportation\\n& Logistics\\nPrimary \\nMarkets\\n15'), Document(id='9305c1f5-e473-4964-99ae-e1be189992ee', metadata={'author': 'Maurice Igugu', 'creationdate': '2019-03-22T15:41:09+01:00', 'creator': 'CorelDRAW 2019', 'moddate': '2019-03-24T15:34:42+01:00', 'page': 21.0, 'page_label': '22', 'producer': 'Corel PDF Engine Version 21.0.0.593', 'source': 'documents\\\\At-The-HEART-of-Sterling.pdf', 'title': 'At The HEART of Sterling - Old March 2019.cdr', 'total_pages': 28.0}, page_content=\"Switch: Our goal is to build a one-stop financial hub for \\ncustomers to initiate and seamlessly complete both \\nfinancial and non-financial activities online or using \\nmobile devices solely with a minimal number (2/3 max) \\nof clicks/taps. The long-term plan for Switch is to \\nbecome that intuitive personalized customer solution \\nthat is constantly adaptive to the customer's \\nbehavioral changes as well as provide an ecosystem \\nfor them to experience their lifestyle choices. \\nSAF Retail:  The retail store provides us the \\nopportunity to create access to market for our small \\nand medium enterprises through the marketplace, \\nwhile ensuring customers can access credit and the \\nitems they require conveniently. More interestingly is \\nthe adoption of Non-Interest Banking principles in \\ngranting loans.\\nIvest: The growth of the middle class in Nigeria \\nprovides the opportunity for growing investment \\nportfolios as many people are focusing on planning.\")]\n",
      "The products available are a one-stop financial hub for customers to initiate and seamlessly complete both financial and non-financial activities online or using mobile devices, a retail store that provides access to market for small and medium enterprises through the marketplace, and investment portfolios for the growing middle class in Nigeria.\n"
     ]
    }
   ],
   "source": [
    "our_query = \"What are the products available?\"\n",
    "answer = retrieve_answers(our_query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the products available?\n",
    "our_query = \"What are the performance indicators for the agriculture sector?\"\n",
    "answer = retrieve_answers(our_query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved documents: [Document(id='0dffe3b5-1381-42e4-8eec-7aba933619aa', metadata={'author': 'Maurice Igugu', 'creationdate': '2019-03-22T15:41:09+01:00', 'creator': 'CorelDRAW 2019', 'moddate': '2019-03-24T15:34:42+01:00', 'page': 0.0, 'page_label': '1', 'producer': 'Corel PDF Engine Version 21.0.0.593', 'source': 'documents\\\\At-The-HEART-of-Sterling.pdf', 'title': 'At The HEART of Sterling - Old March 2019.cdr', 'total_pages': 28.0}, page_content='At The Heart Of Sterling'), Document(id='5f0651ac-7c67-49ac-8a76-eb19bbfb5b68', metadata={'author': 'Maurice Igugu', 'creationdate': '2019-03-22T15:41:09+01:00', 'creator': 'CorelDRAW 2019', 'moddate': '2019-03-24T15:34:42+01:00', 'page': 21.0, 'page_label': '22', 'producer': 'Corel PDF Engine Version 21.0.0.593', 'source': 'documents\\\\At-The-HEART-of-Sterling.pdf', 'title': 'At The HEART of Sterling - Old March 2019.cdr', 'total_pages': 28.0}, page_content='digitization are to:\\nSpecta is an online community lending solution \\ntargeted at providing quick loans up to N5,000,000 \\nto employees of pre-qualified organizations. With \\nSpecta, customers do not need collateral or \\npaperwork to get any loans. Once application is \\ncomplete, scoring is instant and disbursement done in \\nless than 5 minutes. \\nThis is a lifestyle payment platform that enables users \\naccess banking without leaving their social space. \\nThrough chatbots and artificial intelligence, customers \\ncan carry out financial transactions easily.\\nŸ Improve efficiency and optimize our cost lines\\nŸ Focus on the core markets we are distinguishing \\nourselves in and be the first to digitize them.\\nŸ Optimize operations and technology to drive \\nbetter control, manage costs, complexity and risk\\nSwitch: Our goal is to build a one-stop financial hub for \\ncustomers to initiate and seamlessly complete both \\nfinancial and non-financial activities online or using')]\n",
      "\n",
      "The highlights of the provided context are the features of Specta, an online lending solution that offers quick loans without collateral or paperwork, and the goal of building a one-stop financial hub for customers to complete various activities online. Additionally, the focus on digitization and improving efficiency, optimizing cost lines, and utilizing technology to drive better control and manage costs, complexity, and risk are also highlighted.\n"
     ]
    }
   ],
   "source": [
    "# What are the products available?\n",
    "our_query = \"Highlights?\"\n",
    "answer = retrieve_answers(our_query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Using ChatOpenAI for more modern chat-based models\n",
    "# Uncomment below to use ChatOpenAI instead of OpenAI completion model\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# \n",
    "# # Use ChatOpenAI with gpt-3.5-turbo or gpt-4\n",
    "# chat_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "# \n",
    "# # Create a chat prompt template\n",
    "# chat_system_prompt = (\n",
    "#     \"You are an assistant for question-answering tasks. \"\n",
    "#     \"Use the following pieces of retrieved context to answer the question. \"\n",
    "#     \"If you don't know the answer, just say that you don't know. \"\n",
    "#     \"Use three sentences maximum and keep the answer concise.\\n\\n\"\n",
    "#     \"{context}\"\n",
    "# )\n",
    "# \n",
    "# chat_prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", chat_system_prompt),\n",
    "#     (\"human\", \"{input}\"),\n",
    "# ])\n",
    "# \n",
    "# # Create chat-based document processing chain\n",
    "# chat_question_answer_chain = create_stuff_documents_chain(chat_llm, chat_prompt)\n",
    "# \n",
    "# # Create chat-based retrieval chain\n",
    "# chat_rag_chain = create_retrieval_chain(retriever, chat_question_answer_chain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
