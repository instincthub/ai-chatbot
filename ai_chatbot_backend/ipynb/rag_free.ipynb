{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free RAG Implementation\n",
    "## Using HuggingFace Embeddings + Pinecone Free Tier + Free LLM\n",
    "\n",
    "This notebook implements a completely free RAG system for development purposes:\n",
    "- **Embeddings**: HuggingFace (all-MiniLM-L6-v2) - Free\n",
    "- **Vector Store**: Pinecone Free Tier (2GB, 2M writes, 1M reads/month)\n",
    "- **LLM**: HuggingFace Transformers (Flan-T5) - Free local inference\n",
    "- **Document Processing**: Same as original implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries for Free RAG Implementation\n",
    "import pinecone \n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.vectorstores import Pinecone\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pinecone credentials loaded!\n"
     ]
    }
   ],
   "source": [
    "# API Keys (Only Pinecone needed for free tier)\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\")\n",
    "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "\n",
    "# Verify Pinecone credentials\n",
    "if not all([PINECONE_API_KEY, PINECONE_ENVIRONMENT, PINECONE_INDEX_NAME]):\n",
    "    print(\"⚠️ Please set PINECONE_API_KEY, PINECONE_ENVIRONMENT, and PINECONE_INDEX_NAME in your .env file\")\n",
    "else:\n",
    "    print(\"✅ Pinecone credentials loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pinecone initialized with index: ai-chatbot\n"
     ]
    }
   ],
   "source": [
    "# Initialize Pinecone (Free Tier)\n",
    "pinecone.Pinecone(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_ENVIRONMENT\n",
    ")\n",
    "index_name = PINECONE_INDEX_NAME\n",
    "print(f\"✅ Pinecone initialized with index: {index_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading HuggingFace embeddings model...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\instincthub\\code_projects\\ai-projects\\ai-chatbot\\ai_chatbot_backend\\venv\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:84\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sentence_transformers'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## Free HuggingFace Embeddings\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Using sentence-transformers all-MiniLM-L6-v2 model\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# This model is free, fast, and produces good quality embeddings\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m📥 Loading HuggingFace embeddings model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m embeddings = \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mall-MiniLM-L6-v2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdevice\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use 'cuda' if you have GPU\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnormalize_embeddings\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ HuggingFace embeddings loaded!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m📊 Embedding dimension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(embeddings.embed_query(\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\instincthub\\code_projects\\ai-projects\\ai-chatbot\\ai_chatbot_backend\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:222\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    220\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    221\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\instincthub\\code_projects\\ai-projects\\ai-chatbot\\ai_chatbot_backend\\venv\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:87\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     88\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import sentence_transformers python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     89\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     90\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;28mself\u001b[39m.client = sentence_transformers.SentenceTransformer(\n\u001b[32m     93\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_name, cache_folder=\u001b[38;5;28mself\u001b[39m.cache_folder, **\u001b[38;5;28mself\u001b[39m.model_kwargs\n\u001b[32m     94\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`."
     ]
    }
   ],
   "source": [
    "## Free HuggingFace Embeddings\n",
    "# Using sentence-transformers all-MiniLM-L6-v2 model\n",
    "# This model is free, fast, and produces good quality embeddings\n",
    "print(\"📥 Loading HuggingFace embeddings model...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},  # Use 'cuda' if you have GPU\n",
    "    encode_kwargs={'normalize_embeddings': True},\n",
    ")\n",
    "print(\"✅ HuggingFace embeddings loaded!\")\n",
    "print(f\"📊 Embedding dimension: {len(embeddings.embed_query('test'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Document Loading (Same as original)\n",
    "def read_doc(directory):\n",
    "    \"\"\"Load PDF documents from directory\"\"\"\n",
    "    file_loader = PyPDFDirectoryLoader(directory)\n",
    "    documents = file_loader.load()\n",
    "    return documents\n",
    "\n",
    "print(\"📚 Loading documents...\")\n",
    "doc = read_doc('documents/')\n",
    "print(f\"✅ Loaded {len(doc)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text Chunking (Same as original)\n",
    "def chunk_data(docs, chunk_size=800, chunk_overlap=50):\n",
    "    \"\"\"Split documents into chunks for better retrieval\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    doc = text_splitter.split_documents(docs)\n",
    "    return doc\n",
    "\n",
    "print(\"✂️ Splitting documents into chunks...\")\n",
    "documents = chunk_data(docs=doc)\n",
    "print(f\"✅ Created {len(documents)} document chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test embedding generation\n",
    "print(\"🧪 Testing embedding generation...\")\n",
    "test_vectors = embeddings.embed_query(\"How are you?\")\n",
    "print(f\"✅ Generated embedding vector of length: {len(test_vectors)}\")\n",
    "print(f\"📈 Sample values: {test_vectors[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pinecone vector index using free HuggingFace embeddings\n",
    "print(\"🔄 Creating Pinecone vector index with free embeddings...\")\n",
    "print(\"⏳ This may take a few minutes for large documents...\")\n",
    "\n",
    "index = Pinecone.from_documents(\n",
    "    documents, \n",
    "    embeddings, \n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "print(\"✅ Vector index created successfully!\")\n",
    "print(f\"📊 Indexed {len(documents)} document chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Free Local LLM Setup\n",
    "# Using Google's Flan-T5 model - free and runs locally\n",
    "print(\"🤖 Loading free LLM (Flan-T5)...\")\n",
    "print(\"⏳ First time loading may take a few minutes...\")\n",
    "\n",
    "model_name = \"google/flan-t5-base\"  # ~250MB model\n",
    "# For better quality but larger size, use: \"google/flan-t5-large\" (~800MB)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Create pipeline for text generation\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    temperature=0.3,\n",
    "    do_sample=True,\n",
    "    device=-1  # Use CPU, change to 0 for GPU\n",
    ")\n",
    "\n",
    "# Create LangChain LLM\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "print(\"✅ Free LLM loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create QA chain with free LLM\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "print(\"🔗 QA chain created with free LLM!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieval Functions (Same as original)\n",
    "def retrieve_query(query, k=2):\n",
    "    \"\"\"Retrieve relevant documents using cosine similarity\"\"\"\n",
    "    matching_results = index.similarity_search(query, k=k)\n",
    "    return matching_results\n",
    "\n",
    "def retrieve_answers(query):\n",
    "    \"\"\"Get answers using free RAG pipeline\"\"\"\n",
    "    print(f\"🔍 Query: {query}\")\n",
    "    print(\"📚 Retrieving relevant documents...\")\n",
    "    \n",
    "    doc_search = retrieve_query(query)\n",
    "    print(f\"✅ Found {len(doc_search)} relevant documents\")\n",
    "    \n",
    "    print(\"🤖 Generating answer with free LLM...\")\n",
    "    response = chain.run(input_documents=doc_search, question=query)\n",
    "    \n",
    "    return response, doc_search\n",
    "\n",
    "print(\"🛠️ Retrieval functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the Free RAG System\n",
    "our_query = \"How much the agriculture target will be increased by how many crore?\"\n",
    "\n",
    "print(\"🚀 Testing Free RAG System\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "answer, source_docs = retrieve_answers(our_query)\n",
    "\n",
    "print(\"\\n📋 ANSWER:\")\n",
    "print(\"-\" * 20)\n",
    "print(answer)\n",
    "\n",
    "print(\"\\n📚 SOURCE DOCUMENTS:\")\n",
    "print(\"-\" * 30)\n",
    "for i, doc in enumerate(source_docs, 1):\n",
    "    print(f\"\\n📄 Document {i}:\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Page: {doc.metadata.get('page', 'Unknown')}\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interactive Query Function\n",
    "def ask_question(question):\n",
    "    \"\"\"Interactive function to ask questions\"\"\"\n",
    "    print(f\"\\n❓ Question: {question}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    answer, docs = retrieve_answers(question)\n",
    "    \n",
    "    print(f\"\\n💡 Answer: {answer}\")\n",
    "    print(f\"\\n📊 Based on {len(docs)} relevant document(s)\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test with different questions\n",
    "questions = [\n",
    "    \"What is the agriculture credit target?\",\n",
    "    \"What initiatives are mentioned for farmers?\",\n",
    "    \"Tell me about Shree Anna\"\n",
    "]\n",
    "\n",
    "print(\"🎯 Testing multiple queries:\")\n",
    "for q in questions:\n",
    "    ask_question(q)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Free RAG System Summary\n",
    "\n",
    "### ✅ What We Achieved:\n",
    "- **100% Free RAG Implementation** for development\n",
    "- **HuggingFace Embeddings**: High-quality, free embeddings\n",
    "- **Pinecone Free Tier**: Cloud vector storage (2GB limit)\n",
    "- **Local LLM**: Flan-T5 running without API costs\n",
    "- **Same Functionality**: Document processing, chunking, and Q&A\n",
    "\n",
    "### 💰 Cost Comparison:\n",
    "- **Original**: OpenAI embeddings + OpenAI LLM = $$ per query\n",
    "- **Free Version**: $0 for unlimited local queries\n",
    "- **Only Cost**: Pinecone free tier (sufficient for development)\n",
    "\n",
    "### 🚀 Next Steps:\n",
    "1. **Production**: Can easily switch to paid services when needed\n",
    "2. **Scaling**: Upgrade Pinecone or use local vector stores\n",
    "3. **Performance**: Use larger models or GPU acceleration\n",
    "4. **Integration**: Connect with Django backend using these components\n",
    "\n",
    "### 🔧 Environment Variables Needed:\n",
    "```env\n",
    "PINECONE_API_KEY=your_free_pinecone_key\n",
    "PINECONE_ENVIRONMENT=your_pinecone_environment\n",
    "PINECONE_INDEX_NAME=your_index_name\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
